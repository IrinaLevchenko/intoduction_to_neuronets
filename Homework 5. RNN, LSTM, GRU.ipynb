{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ace212",
   "metadata": {},
   "source": [
    "1.Попробуйте изменить параметры нейронной сети, работающей с датасетом imdb, либо нейронной сети, работающей airline-passengers (она прилагается вместе с датасетом к уроку в виде отдельного скрипта) так, чтобы улучшить её точность. Приложите анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db38022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Построение модели...\n",
      "Загрузка данных...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 4s 0us/step\n",
      "25000 тренировочные последовательности\n",
      "25000 тестовые последовательности\n",
      "Pad последовательности (примеров в x единицу времени)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Процесс обучения...\n",
      "Epoch 1/12\n",
      "157/157 [==============================] - 146s 894ms/step - loss: 0.6932 - accuracy: 0.4967 - val_loss: 0.6931 - val_accuracy: 0.5171\n",
      "Epoch 2/12\n",
      "157/157 [==============================] - 149s 948ms/step - loss: 0.6931 - accuracy: 0.5039 - val_loss: 0.6931 - val_accuracy: 0.5217\n",
      "Epoch 3/12\n",
      "157/157 [==============================] - 150s 958ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5199\n",
      "Epoch 4/12\n",
      "157/157 [==============================] - 150s 956ms/step - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6930 - val_accuracy: 0.5185\n",
      "Epoch 5/12\n",
      "157/157 [==============================] - 152s 968ms/step - loss: 0.6931 - accuracy: 0.4975 - val_loss: 0.6930 - val_accuracy: 0.5114\n",
      "Epoch 6/12\n",
      "157/157 [==============================] - 151s 962ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6930 - val_accuracy: 0.5027\n",
      "Epoch 7/12\n",
      "157/157 [==============================] - 164s 1s/step - loss: 0.6930 - accuracy: 0.5052 - val_loss: 0.6929 - val_accuracy: 0.5278\n",
      "Epoch 8/12\n",
      "157/157 [==============================] - 156s 993ms/step - loss: 0.6929 - accuracy: 0.5123 - val_loss: 0.6929 - val_accuracy: 0.5233\n",
      "Epoch 9/12\n",
      "157/157 [==============================] - 155s 990ms/step - loss: 0.6929 - accuracy: 0.5128 - val_loss: 0.6929 - val_accuracy: 0.5324\n",
      "Epoch 10/12\n",
      "157/157 [==============================] - 154s 981ms/step - loss: 0.6929 - accuracy: 0.5107 - val_loss: 0.6928 - val_accuracy: 0.5273\n",
      "Epoch 11/12\n",
      "157/157 [==============================] - 153s 973ms/step - loss: 0.6928 - accuracy: 0.5104 - val_loss: 0.6928 - val_accuracy: 0.5249\n",
      "Epoch 12/12\n",
      "157/157 [==============================] - 278s 2s/step - loss: 0.6928 - accuracy: 0.5112 - val_loss: 0.6927 - val_accuracy: 0.5281\n",
      "157/157 [==============================] - 14s 87ms/step - loss: 0.6927 - accuracy: 0.5281\n",
      "Результат при тестировании: 0.6926944255828857\n",
      "Тестовая точность: 0.5281199812889099\n",
      "CPU times: total: 1h 27min 55s\n",
      "Wall time: 32min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "max_features = 10000\n",
    "\n",
    "# обрезание текстов после данного количества слов (среди top max_features наиболее используемые слова)\n",
    "maxlen = 100\n",
    "batch_size = 160 # увеличьте значение для ускорения обучения\n",
    "\n",
    "def train_nn():\n",
    "    print('Построение модели...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 160))\n",
    "    model.add(LSTM(160, dropout=0.4, recurrent_dropout=0.4))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(16, activation='LeakyReLU'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print('Загрузка данных...')\n",
    "    #стоит попробовать использовать другие оптимайзер и другие конфигурации оптимайзеров\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='SGD', # при использовании этого оптимайзера модель показывает наилучшие результаты.\n",
    "                  metrics=['accuracy'])\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "    print(len(x_train), 'тренировочные последовательности')\n",
    "    print(len(x_test), 'тестовые последовательности')\n",
    "\n",
    "    print('Pad последовательности (примеров в x единицу времени)')\n",
    "    x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('x_test shape:', x_test.shape)\n",
    "    print('Процесс обучения...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=12, # увеличьте при необходимости\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "    print('Результат при тестировании:', score)\n",
    "    print('Тестовая точность:', acc)\n",
    "\n",
    "\n",
    "\n",
    "train_nn()\n",
    "  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f3c1b",
   "metadata": {},
   "source": [
    "Тестовая точность: 0.528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290b4988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Построение модели...\n",
      "Загрузка данных...\n",
      "25000 тренировочные последовательности\n",
      "25000 тестовые последовательности\n",
      "Pad последовательности (примеров в x единицу времени)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Процесс обучения...\n",
      "Epoch 1/15\n",
      "157/157 [==============================] - 149s 922ms/step - loss: 0.5100 - accuracy: 0.7410 - val_loss: 0.3747 - val_accuracy: 0.8332\n",
      "Epoch 2/15\n",
      "157/157 [==============================] - 144s 919ms/step - loss: 0.3114 - accuracy: 0.8800 - val_loss: 0.3427 - val_accuracy: 0.8518\n",
      "Epoch 3/15\n",
      "157/157 [==============================] - 145s 923ms/step - loss: 0.2488 - accuracy: 0.9088 - val_loss: 0.3606 - val_accuracy: 0.8465\n",
      "Epoch 4/15\n",
      "157/157 [==============================] - 144s 915ms/step - loss: 0.1966 - accuracy: 0.9296 - val_loss: 0.3873 - val_accuracy: 0.8378\n",
      "Epoch 5/15\n",
      "157/157 [==============================] - 145s 923ms/step - loss: 0.1587 - accuracy: 0.9449 - val_loss: 0.5275 - val_accuracy: 0.8399\n",
      "Epoch 6/15\n",
      "157/157 [==============================] - 144s 916ms/step - loss: 0.1199 - accuracy: 0.9583 - val_loss: 0.5072 - val_accuracy: 0.8256\n",
      "Epoch 7/15\n",
      "157/157 [==============================] - 144s 921ms/step - loss: 0.1021 - accuracy: 0.9653 - val_loss: 0.5978 - val_accuracy: 0.8272\n",
      "Epoch 8/15\n",
      "157/157 [==============================] - 144s 919ms/step - loss: 0.0801 - accuracy: 0.9734 - val_loss: 0.6785 - val_accuracy: 0.8236\n",
      "Epoch 9/15\n",
      "157/157 [==============================] - 144s 920ms/step - loss: 0.0719 - accuracy: 0.9755 - val_loss: 0.7632 - val_accuracy: 0.8273\n",
      "Epoch 10/15\n",
      "157/157 [==============================] - 145s 925ms/step - loss: 0.0586 - accuracy: 0.9819 - val_loss: 0.8096 - val_accuracy: 0.8248\n",
      "Epoch 11/15\n",
      "157/157 [==============================] - 145s 924ms/step - loss: 0.0531 - accuracy: 0.9826 - val_loss: 0.9759 - val_accuracy: 0.8280\n",
      "Epoch 12/15\n",
      "157/157 [==============================] - 161s 1s/step - loss: 0.0455 - accuracy: 0.9850 - val_loss: 1.1081 - val_accuracy: 0.8106\n",
      "Epoch 13/15\n",
      "157/157 [==============================] - 144s 918ms/step - loss: 0.0566 - accuracy: 0.9818 - val_loss: 0.9377 - val_accuracy: 0.8270\n",
      "Epoch 14/15\n",
      "157/157 [==============================] - 144s 919ms/step - loss: 0.0362 - accuracy: 0.9887 - val_loss: 0.9549 - val_accuracy: 0.8175\n",
      "Epoch 15/15\n",
      "157/157 [==============================] - 147s 935ms/step - loss: 0.0334 - accuracy: 0.9900 - val_loss: 1.0072 - val_accuracy: 0.8268\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 1.0072 - accuracy: 0.8268\n",
      "Результат при тестировании: 1.0072365999221802\n",
      "Тестовая точность: 0.8268399834632874\n",
      "CPU times: total: 1h 47min 9s\n",
      "Wall time: 36min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "max_features = 10000\n",
    "\n",
    "# обрезание текстов после данного количества слов (среди top max_features наиболее используемые слова)\n",
    "maxlen = 100\n",
    "batch_size = 160 # увеличьте значение для ускорения обучения\n",
    "\n",
    "def train_nn():\n",
    "    print('Построение модели...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 160))\n",
    "    model.add(LSTM(160, dropout=0.4, recurrent_dropout=0.4))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(32, activation='tanh'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(16, activation='LeakyReLU'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print('Загрузка данных...')\n",
    "    #стоит попробовать использовать другие оптимайзер и другие конфигурации оптимайзеров\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='Adam', # при использовании этого оптимайзера модель показывает наилучшие результаты.\n",
    "                  metrics=['accuracy'])\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "    print(len(x_train), 'тренировочные последовательности')\n",
    "    print(len(x_test), 'тестовые последовательности')\n",
    "\n",
    "    print('Pad последовательности (примеров в x единицу времени)')\n",
    "    x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('x_test shape:', x_test.shape)\n",
    "    print('Процесс обучения...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=15, # увеличьте при необходимости\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "    print('Результат при тестировании:', score)\n",
    "    print('Тестовая точность:', acc)\n",
    "\n",
    "\n",
    "\n",
    "train_nn()\n",
    "  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc8985",
   "metadata": {},
   "source": [
    " Тестовая точность: 0.826"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2f6dc",
   "metadata": {},
   "source": [
    "Вывод:\n",
    "добавление полносвязных скрытых слоёв не улучшило точность нейросети, а вот изменение оптимизатора и функций активации - да."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2017a123",
   "metadata": {},
   "source": [
    "2. Попробуйте изменить параметры нейронной сети, генерирующей текст, таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший получившийся у вас текст и опишите то, что вы предприняли, чтобы его получить. Можно использовать текст другого прозведения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c7940d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Bidirectional\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d8371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# построчное чтение из примера с текстом\n",
    "with open(\"alice_in_wonderland.txt\", 'rb') as _in:\n",
    "    lines = []\n",
    "    for line in _in:\n",
    "        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        lines.append(line)\n",
    "text = \" \".join(lines)\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "\n",
    "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
    "# ID and a specific character. The numerical ID will correspond to a column\n",
    "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
    "# число при использовании one-hot кодировки для представление входов символов\n",
    "char2index = {c: i for i, c in enumerate(chars)}\n",
    "index2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# для удобства выберете фиксированную длину последовательность 10 символов\n",
    "SEQLEN, STEP = 10, 1\n",
    "input_chars, label_chars = [], []\n",
    "\n",
    "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i: i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n",
    "\n",
    "\n",
    "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
    "\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bdc85b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ff514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# установка ряда метапамертров  для нейронной сети и процесса тренировки\n",
    "BATCH_SIZE, HIDDEN_SIZE = 128, 128\n",
    "NUM_ITERATIONS = 15 # 25 должно быть достаточно\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c7b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Итерация #: 0\n",
      "1241/1241 [==============================] - 51s 25ms/step - loss: 2.6812\n",
      "Генерация из посева: f course, \n",
      "f course, the the the the the the the the the the the the the the the the the the the the the the the the the ==================================================\n",
      "Итерация #: 1\n",
      "1241/1241 [==============================] - 31s 25ms/step - loss: 2.4111\n",
      "Генерация из посева: its him. i\n",
      "its him. in the she the the the the the the the the the the the the the the the the the the the the the the th==================================================\n",
      "Итерация #: 2\n",
      "1241/1241 [==============================] - 27s 21ms/step - loss: 2.3057\n",
      "Генерация из посева: autifully \n",
      "autifully and the said the said the said the said the said the said the said the said the said the said the sa==================================================\n",
      "Итерация #: 3\n",
      "1241/1241 [==============================] - 22s 18ms/step - loss: 2.2278\n",
      "Генерация из посева: o stand on\n",
      "o stand on the said the said the said the said the said the said the said the said the said the said the said ==================================================\n",
      "Итерация #: 4\n",
      "1241/1241 [==============================] - 21s 17ms/step - loss: 2.1685\n",
      "Генерация из посева: tronic wor\n",
      "tronic wor the said the said the said the said the said the said the said the said the said the said the said ==================================================\n",
      "Итерация #: 5\n",
      "1241/1241 [==============================] - 19s 16ms/step - loss: 2.1199\n",
      "Генерация из посева: g else to \n",
      "g else to the said the was the said the was the said the was the said the was the said the was the said the wa==================================================\n",
      "Итерация #: 6\n",
      "1241/1241 [==============================] - 19s 16ms/step - loss: 2.0833\n",
      "Генерация из посева: eat a bat?\n",
      "eat a bat? and the was the was the was the was the was the was the was the was the was the was the was the was==================================================\n",
      "Итерация #: 7\n",
      "1241/1241 [==============================] - 20s 16ms/step - loss: 2.0488\n",
      "Генерация из посева: . williams\n",
      ". williams the was the was the was the was the was the was the was the was the was the was the was the was the==================================================\n",
      "Итерация #: 8\n",
      "1241/1241 [==============================] - 20s 16ms/step - loss: 2.0208\n",
      "Генерация из посева: , (801) 59\n",
      ", (801) 59e the mark the mark the mark the mark the mark the mark the mark the mark the mark the mark the mark==================================================\n",
      "Итерация #: 9\n",
      "1241/1241 [==============================] - 20s 16ms/step - loss: 1.9975\n",
      "Генерация из посева:  as she co\n",
      " as she could the mark the mark the mark the mark the mark the mark the mark the mark the mark the mark the ma==================================================\n",
      "Итерация #: 10\n",
      "1241/1241 [==============================] - 20s 16ms/step - loss: 1.9766\n",
      "Генерация из посева:  anxiously\n",
      " anxiously and the was the was the was the was the was the was the was the was the was the was the was the was==================================================\n",
      "Итерация #: 11\n",
      "1241/1241 [==============================] - 20s 16ms/step - loss: 1.9568\n",
      "Генерация из посева: ther offen\n",
      "ther offen the mark the mark the mark the mark the mark the mark the mark the mark the mark the mark the mark ==================================================\n",
      "Итерация #: 12\n",
      "1241/1241 [==============================] - 20s 16ms/step - loss: 1.9409\n",
      "Генерация из посева: ld not hel\n",
      "ld not hell the mark the mark the mark the mark the mark the mark the mark the mark the mark the mark the mark==================================================\n",
      "Итерация #: 13\n",
      "1241/1241 [==============================] - 20s 16ms/step - loss: 1.9268\n",
      "Генерация из посева:  licking h\n",
      " licking her she said the groject gutenberg-to the groject gutenberg-to the groject gutenberg-to the groject g==================================================\n",
      "Итерация #: 14\n",
      "1241/1241 [==============================] - 20s 16ms/step - loss: 1.9111\n",
      "Генерация из посева: es behind \n",
      "es behind to the mork to the mork to the mork to the mork to the mork to the mork to the mork to the mork to t\n",
      "CPU times: total: 13min 57s\n",
      "Wall time: 8min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create a super simple recurrent neural network. There is one recurrent\n",
    "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
    "# encoded input layer. This is followed by a Dense fully-connected layer\n",
    "# across the set of possible next characters, which is converted to a\n",
    "# probability score via a standard softmax activation with a multi-class\n",
    "# cross-entropy loss function linking the prediction to the one-hot\n",
    "# encoding character label.\n",
    "\n",
    "'''\n",
    "Создание очень простой рекуррентной нейронной сети.\n",
    "В ней будет один реккурентный закодированный входной слой. \n",
    "За ним последует полносвязный слой связанный с набором возможных \n",
    "следующих символов, которые конвертированы в вероятностные результаты \n",
    "через стандартную softmax активацию с multi-class cross-encoding loss \n",
    "функцию ссылающуются на предсказание one-hot encoding лейбл символа\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
    "        110,\n",
    "        dropout=0.4,\n",
    "        recurrent_dropout=0.4, implementation=2,\n",
    "        input_shape=(SEQLEN, nb_chars),\n",
    "        unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "# выполнение серий тренировочных и демонстрационных итераций\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # для каждой итерации запуск передачи данных в модель\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Итерация #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "\n",
    "\n",
    "    # для числа шагов предсказаний использование текущей тренируемой модели\n",
    "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
    "    print(\"Генерация из посева: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # здесь one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, char2index[ch]] = 1\n",
    "\n",
    "        # осуществление предсказания с помощью текущей модели.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        y_pred = index2char[np.argmax(pred)]\n",
    "\n",
    "        # вывод предсказания добавленного к тестовому примеру\n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # инкрементация тестового примера содержащего предсказание\n",
    "        test_chars = test_chars[1:] + y_pred\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93caa24a",
   "metadata": {},
   "source": [
    "Вывод: Изменение GRU на LSTM не дало улучшения результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064aee4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
